{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yang-Heewon/CV_submission/blob/main/Tutorial_Prompt_Optimization_ipynb%EC%9D%98_%EC%B5%9C%EC%A2%85%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fecb2fb5-b87f-4428-8175-e3a46fe77371",
      "metadata": {
        "id": "fecb2fb5-b87f-4428-8175-e3a46fe77371"
      },
      "source": [
        "## Tutorial: Optimizing a Prompt\n",
        "\n",
        "![TextGrad](https://github.com/vinid/data/blob/master/logo_full.png?raw=true)\n",
        "\n",
        "An autograd engine -- for textual gradients!\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zou-group/TextGrad/blob/main/examples/notebooks/Prompt-Optimization.ipynb)\n",
        "[![GitHub license](https://img.shields.io/badge/License-MIT-blue.svg)](https://lbesson.mit-license.org/)\n",
        "[![Arxiv](https://img.shields.io/badge/arXiv-2406.07496-B31B1B.svg)](https://arxiv.org/abs/2406.07496)\n",
        "[![Documentation Status](https://readthedocs.org/projects/textgrad/badge/?version=latest)](https://textgrad.readthedocs.io/en/latest/?badge=latest)\n",
        "[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/textgrad)](https://pypi.org/project/textgrad/)\n",
        "[![PyPI](https://img.shields.io/pypi/v/textgrad)](https://pypi.org/project/textgrad/)\n",
        "\n",
        "**Objectives:**\n",
        "\n",
        "* In this tutorial, we will run prompt optimization.\n",
        "\n",
        "**Requirements:**\n",
        "\n",
        "* You need to have an OpenAI API key to run this tutorial. This should be set as an environment variable as OPENAI_API_KEY.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7add4547-4278-411b-a827-79be521851f1",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-11T19:30:34.029594610Z",
          "start_time": "2024-06-11T19:30:34.024175489Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7add4547-4278-411b-a827-79be521851f1",
        "outputId": "fbd5268a-218a-47e6-988b-bd07349e5f0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textgrad in /usr/local/lib/python3.12/dist-packages (0.1.8)\n",
            "Requirement already satisfied: openai>=1.23.6 in /usr/local/lib/python3.12/dist-packages (from textgrad) (1.109.1)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from textgrad) (8.5.0)\n",
            "Requirement already satisfied: python-dotenv>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from textgrad) (1.1.1)\n",
            "Requirement already satisfied: pandas>=1.5.3 in /usr/local/lib/python3.12/dist-packages (from textgrad) (2.2.2)\n",
            "Requirement already satisfied: platformdirs>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from textgrad) (4.5.0)\n",
            "Requirement already satisfied: datasets>=2.14.6 in /usr/local/lib/python3.12/dist-packages (from textgrad) (4.0.0)\n",
            "Requirement already satisfied: diskcache>=5.6.3 in /usr/local/lib/python3.12/dist-packages (from textgrad) (5.6.3)\n",
            "Requirement already satisfied: graphviz>=0.20.3 in /usr/local/lib/python3.12/dist-packages (from textgrad) (0.21)\n",
            "Requirement already satisfied: gdown>=5.2.0 in /usr/local/lib/python3.12/dist-packages (from textgrad) (5.2.0)\n",
            "Requirement already satisfied: litellm>=1.49.5 in /usr/local/lib/python3.12/dist-packages (from textgrad) (1.78.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from textgrad) (11.3.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from textgrad) (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->textgrad) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->textgrad) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->textgrad) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->textgrad) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->textgrad) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->textgrad) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->textgrad) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->textgrad) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->textgrad) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->textgrad) (0.35.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->textgrad) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.6->textgrad) (6.0.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown>=5.2.0->textgrad) (4.13.5)\n",
            "Requirement already satisfied: aiohttp>=3.10 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.49.5->textgrad) (3.13.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from litellm>=1.49.5->textgrad) (8.3.0)\n",
            "Requirement already satisfied: fastuuid>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.49.5->textgrad) (0.13.5)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.49.5->textgrad) (8.7.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.49.5->textgrad) (3.1.6)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.49.5->textgrad) (4.25.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.49.5->textgrad) (2.11.10)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.49.5->textgrad) (0.12.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (from litellm>=1.49.5->textgrad) (0.22.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx->textgrad) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx->textgrad) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->textgrad) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx->textgrad) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->textgrad) (0.16.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.23.6->textgrad) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.23.6->textgrad) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.23.6->textgrad) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai>=1.23.6->textgrad) (4.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.3->textgrad) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.3->textgrad) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.3->textgrad) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.49.5->textgrad) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.49.5->textgrad) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.49.5->textgrad) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.49.5->textgrad) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.49.5->textgrad) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.49.5->textgrad) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.49.5->textgrad) (1.22.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets>=2.14.6->textgrad) (1.1.10)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=6.8.0->litellm>=1.49.5->textgrad) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.49.5->textgrad) (3.0.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.49.5->textgrad) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.49.5->textgrad) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.49.5->textgrad) (0.27.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.5.0->litellm>=1.49.5->textgrad) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.5.0->litellm>=1.49.5->textgrad) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.5.0->litellm>=1.49.5->textgrad) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.3->textgrad) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.14.6->textgrad) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.14.6->textgrad) (2.5.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.7.0->litellm>=1.49.5->textgrad) (2024.11.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown>=5.2.0->textgrad) (2.8)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown>=5.2.0->textgrad) (1.7.1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "!pip install textgrad # you might need to restart the notebook after installing textgrad\n",
        "\n",
        "import argparse\n",
        "import concurrent\n",
        "from dotenv import load_dotenv\n",
        "from tqdm import tqdm\n",
        "import textgrad as tg\n",
        "from textgrad.tasks import load_task\n",
        "import numpy as np\n",
        "import random\n",
        "load_dotenv(override=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a459a37-7446-4c4a-a7e0-38182b5dbd3e",
      "metadata": {
        "id": "9a459a37-7446-4c4a-a7e0-38182b5dbd3e"
      },
      "source": [
        "Let's first define some support functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ccc3b21bf9ddc48",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-11T19:30:42.098338405Z",
          "start_time": "2024-06-11T19:30:42.093473103Z"
        },
        "id": "1ccc3b21bf9ddc48"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "649e06aef34d0990",
      "metadata": {
        "id": "649e06aef34d0990"
      },
      "outputs": [],
      "source": [
        "def eval_sample(item, eval_fn, model):\n",
        "    \"\"\"\n",
        "    This function allows us to evaluate if an answer to a question in the prompt is a good answer.\n",
        "\n",
        "    \"\"\"\n",
        "    x, y = item\n",
        "    x = tg.Variable(x, requires_grad=False, role_description=\"query to the language model\")\n",
        "    y = tg.Variable(y, requires_grad=False, role_description=\"correct answer for the query\")\n",
        "    response = model(x)\n",
        "    try:\n",
        "        eval_output_variable = eval_fn(inputs=dict(prediction=response, ground_truth_answer=y))\n",
        "        return int(eval_output_variable.value)\n",
        "    except:\n",
        "        eval_output_variable = eval_fn([x, y, response])\n",
        "        eval_output_parsed = eval_fn.parse_output(eval_output_variable)\n",
        "        return int(eval_output_parsed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9559a31e07e54d7f",
      "metadata": {
        "id": "9559a31e07e54d7f"
      },
      "outputs": [],
      "source": [
        "def eval_dataset(test_set, eval_fn, model, max_samples: int=None):\n",
        "    if max_samples is None:\n",
        "        max_samples = len(test_set)\n",
        "    accuracy_list = []\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
        "        futures = []\n",
        "        for _, sample in enumerate(test_set):\n",
        "\n",
        "            future = executor.submit(eval_sample, sample, eval_fn, model)\n",
        "            futures.append(future)\n",
        "            if len(futures) >= max_samples:\n",
        "                break\n",
        "        tqdm_loader = tqdm(concurrent.futures.as_completed(futures), total=len(futures), position=0)\n",
        "        for future in tqdm_loader:\n",
        "            acc_item = future.result()\n",
        "            accuracy_list.append(acc_item)\n",
        "            tqdm_loader.set_description(f\"Accuracy: {np.mean(accuracy_list)}\")\n",
        "    return accuracy_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ea732b7edf34eb9",
      "metadata": {
        "id": "4ea732b7edf34eb9"
      },
      "outputs": [],
      "source": [
        "def run_validation_revert(system_prompt: tg.Variable, results, model, eval_fn, val_set):\n",
        "    val_performance = np.mean(eval_dataset(val_set, eval_fn, model))\n",
        "    previous_performance = np.mean(results[\"validation_acc\"][-1])\n",
        "    print(\"val_performance: \", val_performance)\n",
        "    print(\"previous_performance: \", previous_performance)\n",
        "    previous_prompt = results[\"prompt\"][-1]\n",
        "\n",
        "    if val_performance < previous_performance:\n",
        "        print(f\"rejected prompt: {system_prompt.value}\")\n",
        "        system_prompt.set_value(previous_prompt)\n",
        "        val_performance = previous_performance\n",
        "\n",
        "    results[\"validation_acc\"].append(val_performance)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJAn0WLjTZPu",
        "outputId": "cebf5778-3c06-45ab-cfe2-85ed943ff696"
      },
      "id": "eJAn0WLjTZPu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 로드 (예시: pandas 사용)\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import textgrad as tg # TextGrad 임포트 추가\n",
        "\n",
        "# 파일 경로를 실제 데이터셋 파일 경로로 변경하세요.\n",
        "# 초기 로드는 전체 데이터셋으로 간주합니다.\n",
        "full_df = pd.read_excel(\"/content/drive/MyDrive/sample_test.xlsx\")\n",
        "\n",
        "# 데이터셋을 학습(80%), 검증(10%), 테스트(10%) 세트로 분할\n",
        "# 먼저 전체 데이터를 학습+검증 (80%)와 테스트 (20%)로 나눕니다.\n",
        "train_val_df, test_df = train_test_split(full_df, test_size=0.2, random_state=42)\n",
        "\n",
        "# 학습+검증 데이터를 학습 (80% 중 8/9, 즉 전체의 약 71.1%)와 검증 (80% 중 1/9, 즉 전체의 약 8.9%)로 나눕니다.\n",
        "# 8:1:1 비율을 맞추기 위해 test_size를 0.1 / 0.8 = 0.125 로 설정합니다.\n",
        "train_df, val_df = train_test_split(train_val_df, test_size=0.125, random_state=42)\n",
        "\n",
        "\n",
        "# pandas DataFrame을 TextGrad Dataset 형식으로 변환하는 과정이 필요할 수 있습니다.\n",
        "# TextGrad 문서나 예시를 참고하여 적절한 변환 방법을 찾아야 합니다.\n",
        "# train_set = YourTextGradDatasetWrapper(train_df)\n",
        "# val_set = YourTextGradDatasetWrapper(val_df)\n",
        "# test_set = YourTextGradDatasetWrapper(test_df)\n",
        "\n",
        "# 데이터셋에 맞는 평가 함수 정의 (예시)\n",
        "# 이 함수는 모델의 예측(prediction)과 실제 정답(ground_truth)을 비교해야 합니다.\n",
        "def your_evaluation_function(inputs):\n",
        "    try:\n",
        "        prediction = inputs[\"prediction\"].value.strip() # textgrad.Variable에서 값 추출 및 공백 제거\n",
        "        ground_truth_answer = str(inputs[\"ground_truth_answer\"].value).strip() # textgrad.Variable에서 값 추출, 문자열 변환 및 공백 제거\n",
        "    except AttributeError:\n",
        "        # Handle cases where .value might not be available, though less likely with tg.Variable\n",
        "        prediction = str(inputs[\"prediction\"]).strip()\n",
        "        ground_truth_answer = str(inputs[\"ground_truth_answer\"]).strip()\n",
        "    except TypeError:\n",
        "         # This might catch the case where inputs is a list, though the primary fix should prevent this\n",
        "         # For robustness, you could add more specific handling here if needed\n",
        "         return tg.Variable(1, requires_grad=True, role_description=\"Evaluation score\") # Return a loss value for unhandleable cases\n",
        "\n",
        "\n",
        "    # 모델의 예측(prediction)과 실제 정답(ground_truth_answer)이 일치하는지 확인\n",
        "    is_correct = (prediction == ground_truth_answer)\n",
        "\n",
        "    # 결과를 textgrad.Variable로 반환\n",
        "    # TextGrad는 기본적으로 loss를 최소화하므로, 정확도를 최대화하려면\n",
        "    # loss_value = 0 (정답) 또는 1 (오답)으로 설정합니다.\n",
        "    loss_value = 0 if is_correct else 1\n",
        "\n",
        "    return tg.Variable(loss_value, requires_grad=True, role_description=\"Evaluation score\")\n",
        "\n",
        "\n",
        "eval_fn = your_evaluation_function # 정의한 평가 함수 할당\n",
        "\n",
        "# STARTING_SYSTEM_PROMPT도 데이터셋에 맞게 설정해야 합니다.\n",
        "# STARTING_SYSTEM_PROMPT = \"당신의 데이터셋에 맞는 초기 프롬프트\"\n",
        "\n",
        "print(\"Train/Val/Test Set Lengths: \", len(train_df), len(val_df), len(test_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZy9paHxRdjo",
        "outputId": "01ad6906-95fb-4620-90d2-612ae81a4ac6"
      },
      "id": "rZy9paHxRdjo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Val/Test Set Lengths:  13 2 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e69f8431-661c-42f8-b7fc-efccea588a03",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e69f8431-661c-42f8-b7fc-efccea588a03",
        "outputId": "bb7cd157-09b7-4b1e-ffd9-1092e67e25a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Val/Test Set Lengths:  13 2 4\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "set_seed(12)\n",
        "llm_api_eval = tg.get_engine(engine_name=\"gpt-4o\")\n",
        "llm_api_test = tg.get_engine(engine_name=\"gpt-3.5-turbo-0125\")\n",
        "tg.set_backward_engine(llm_api_eval, override=True)\n",
        "\n",
        "# rZy9paHxRdjo 셀에서 로드 및 분할된 데이터프레임을 TextGrad Dataset으로 변환\n",
        "class CustomDataset(tg.tasks.Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.dataframe = dataframe\n",
        "        # 입력으로 review_abstract, review_criteria, 그리고 PICO 정보를 사용하고, label을 정답으로 사용\n",
        "        self.data = [(f\"Review Abstract: {row['review_abstract']}\\nReview Criteria: {row['review_criteria']}\\nPICO: {row['PICO']}\", str(row['label'])) for index, row in dataframe.iterrows()]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "    def get_task_description(self):\n",
        "    # TextGrad 옵티마이저가 새 텍스트를 찾을 수 있도록 태그로 감쌉니다.\n",
        "      return (\n",
        "          \"You are an AI assistant specialized in Systematic Reviews. \"\n",
        "          \"Given a review abstract, review criteria, and PICO elements, \"\n",
        "          \"determine if the paper should be included (label 1) or excluded (label 0) \"\n",
        "          \"based on the criteria and PICO.\\n\"\n",
        "          \"Provide only the label (0 or 1) as your output.\\n\"\n",
        "          \"<TEXT_TO_UPDATE>\\n\"\n",
        "          \"This is the initial prompt for optimization.\\n\"\n",
        "          \"</TEXT_TO_UPDATE>\"\n",
        "      )\n",
        "train_set = CustomDataset(train_df)\n",
        "val_set = CustomDataset(val_df)\n",
        "test_set = CustomDataset(test_df)\n",
        "\n",
        "\n",
        "# rZy9paHxRdjo 셀에서 정의한 평가 함수 사용\n",
        "# eval_fn = your_evaluation_function # rZy9paHxRdjo 셀에서 이미 정의되었으므로 다시 정의할 필요 없음\n",
        "\n",
        "\n",
        "print(\"Train/Val/Test Set Lengths: \", len(train_set), len(val_set), len(test_set))\n",
        "STARTING_SYSTEM_PROMPT = train_set.get_task_description()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f40b576c-4ba0-4e6e-b3ed-81eb44524676",
      "metadata": {
        "id": "f40b576c-4ba0-4e6e-b3ed-81eb44524676"
      },
      "source": [
        "This is the system prompt we are going to start from:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3ed3261-6f9d-4906-8c4b-a3ad570f5950",
      "metadata": {
        "id": "d3ed3261-6f9d-4906-8c4b-a3ad570f5950",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8814c2be-8b1b-43d2-a342-89a833a51899"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an AI assistant specialized in Systematic Reviews. Given a review abstract, review criteria, and PICO elements, determine if the paper should be included (label 1) or excluded (label 0) based on the criteria and PICO.\n",
            "Provide only the label (0 or 1) as your output.\n",
            "<TEXT_TO_UPDATE>\n",
            "This is the initial prompt for optimization.\n",
            "</TEXT_TO_UPDATE>\n"
          ]
        }
      ],
      "source": [
        "print(STARTING_SYSTEM_PROMPT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7544127-38e0-4c74-8632-003efcc645ee",
      "metadata": {
        "id": "f7544127-38e0-4c74-8632-003efcc645ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08321662-5a94-4994-9d4a-5a8cd5f40a07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/4 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:  25%|██▌       | 1/4 [00:01<00:03,  1.19s/it]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 0.5:  50%|█████     | 2/4 [00:01<00:01,  1.50it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 0.3333333333333333:  75%|███████▌  | 3/4 [00:01<00:00,  1.68it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 0.25: 100%|██████████| 4/4 [00:02<00:00,  1.71it/s]\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 0.0:  25%|██▌       | 1/4 [00:00<00:02,  1.30it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 0.5:  50%|█████     | 2/4 [00:00<00:00,  2.47it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 0.6666666666666666:  75%|███████▌  | 3/4 [00:01<00:00,  1.61it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 0.75: 100%|██████████| 4/4 [00:02<00:00,  1.79it/s]\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 0.0:  50%|█████     | 1/2 [00:00<00:00,  1.80it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 0.5: 100%|██████████| 2/2 [00:01<00:00,  1.85it/s]\n"
          ]
        }
      ],
      "source": [
        "train_loader = tg.tasks.DataLoader(train_set, batch_size=3, shuffle=True)\n",
        "\n",
        "\n",
        "# Testing the 0-shot performance of the evaluation engine\n",
        "# system_prompt = tg.Variable(STARTING_SYSTEM_PROMPT,\n",
        "#                             requires_grad=True,\n",
        "#                             role_description=\"system prompt to the language model\")\n",
        "# model_evaluation = tg.BlackboxLLM(llm_api_eval, system_prompt)\n",
        "\n",
        "# system_prompt = tg.Variable(STARTING_SYSTEM_PROMPT,\n",
        "#                             requires_grad=True,\n",
        "#                             role_description=\"structured system prompt to a somewhat capable language model that specifies the behavior and strategies for the QA task\")\n",
        "# model = tg.BlackboxLLM(llm_api_test, system_prompt)\n",
        "# 0-shot 평가용(강한 엔진)\n",
        "system_prompt_eval = tg.Variable(\n",
        "    STARTING_SYSTEM_PROMPT, requires_grad=True,\n",
        "    role_description=\"system prompt to the language model\"\n",
        ")\n",
        "model_evaluation = tg.BlackboxLLM(llm_api_eval, system_prompt_eval)\n",
        "\n",
        "system_prompt_train = tg.Variable(\n",
        "    STARTING_SYSTEM_PROMPT, requires_grad=True,\n",
        "    role_description=(\"structured system prompt to a somewhat capable language model \"\n",
        "                      \"that specifies the behavior and strategies for the QA task\")\n",
        ")\n",
        "model = tg.BlackboxLLM(llm_api_test, system_prompt_train)\n",
        "# optimizer = tg.TextualGradientDescent(engine=llm_api_eval, parameters=[system_prompt])\n",
        "NEW_TAGS = [\"<UPDATED_PROMPT>\", \"</UPDATED_PROMPT>\"]\n",
        "\n",
        "CUSTOM_OPTIMIZER_PROMPT = (\n",
        "    \"You are optimizing a SYSTEM PROMPT for a Systematic Review classifier.\\n\"\n",
        "    \"Return ONLY the improved prompt wrapped EXACTLY as:\\n\"\n",
        "    \"{new_variable_start_tag}<<NEW_PROMPT>>{new_variable_end_tag}\\n\"\n",
        "    \"No explanations. No extra text.\"\n",
        ")\n",
        "# 주의: 위 문자열에는 {new_variable_start_tag}/{new_variable_end_tag}만 사용.\n",
        "# '<<NEW_PROMPT>>'는 단순 예시 문자열이므로 중괄호가 없어 포맷 충돌이 없습니다.\n",
        "\n",
        "optimizer = tg.TextualGradientDescent(\n",
        "    engine=llm_api_eval,\n",
        "    parameters=[system_prompt_train],\n",
        "    new_variable_tags=NEW_TAGS,                 # 리스트/튜플 모두 가능\n",
        "    optimizer_system_prompt=CUSTOM_OPTIMIZER_PROMPT,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# results = {\"test_acc\": [], \"prompt\": [], \"validation_acc\": []}\n",
        "# results[\"test_acc\"].append(eval_dataset(test_set, eval_fn, model))\n",
        "# results[\"validation_acc\"].append(eval_dataset(val_set, eval_fn, model))\n",
        "# results[\"prompt\"].append(system_prompt.get_value())\n",
        "results = {\"test_acc\": [], \"prompt\": [], \"validation_acc\": [], \"zeroshot_eval_acc\":[]}\n",
        "\n",
        "# 0-shot baseline with strong engine (optional)\n",
        "results[\"zeroshot_eval_acc\"].append(eval_dataset(test_set, eval_fn, model_evaluation))\n",
        "\n",
        "# main evals with lightweight engine\n",
        "results[\"test_acc\"].append(eval_dataset(test_set, eval_fn, model))\n",
        "results[\"validation_acc\"].append(eval_dataset(val_set, eval_fn, model))\n",
        "results[\"prompt\"].append(system_prompt_train.get_value())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3807736-1d81-4349-95db-257c20110d1a",
      "metadata": {
        "id": "b3807736-1d81-4349-95db-257c20110d1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f0e7d3b-76a7-4b8f-e967-706a588ac273"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training step 0. Epoch 0: : 0it [00:00, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:TextualGradientDescent prompt for update\n",
            "INFO:textgrad:TextualGradientDescent optimizer response\n",
            "INFO:textgrad:TextualGradientDescent updated text\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:  50%|█████     | 1/2 [00:02<00:02,  2.92s/it]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0: 100%|██████████| 2/2 [00:03<00:00,  1.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val_performance:  1.0\n",
            "previous_performance:  0.5\n",
            "sys prompt:  <<NEW_PROMPT>>You are an AI assistant specialized in Systematic Reviews. Your task is to analyze and synthesize information from various sources to provide comprehensive and accurate answers. Focus on clarity, relevance, and conciseness in your responses. Prioritize evidence-based information and ensure that your answers are well-structured and easy to understand. Use the following strategies: 1) Identify key concepts and terms, 2) Summarize findings from multiple sources, 3) Highlight any consensus or discrepancies, 4) Provide citations where applicable, and 5) Offer a clear conclusion or recommendation based on the evidence. Always aim to enhance the user's understanding and support informed decision-making.<<NEW_PROMPT>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/4 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:  25%|██▌       | 1/4 [00:02<00:07,  2.63s/it]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:  50%|█████     | 2/4 [00:05<00:05,  2.62s/it]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:  75%|███████▌  | 3/4 [00:07<00:02,  2.60s/it]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0: 100%|██████████| 4/4 [00:08<00:00,  2.04s/it]\n",
            "Training step 1. Epoch 0: : 1it [00:16, 16.71s/it]INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:TextualGradientDescent prompt for update\n",
            "INFO:textgrad:TextualGradientDescent optimizer response\n",
            "INFO:textgrad:TextualGradientDescent updated text\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:  50%|█████     | 1/2 [00:04<00:04,  4.24s/it]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0: 100%|██████████| 2/2 [00:05<00:00,  2.67s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val_performance:  1.0\n",
            "previous_performance:  1.0\n",
            "sys prompt:  <<NEW_PROMPT>>You are an AI assistant specialized in Systematic Reviews. Your primary role is to assist users by providing clear, concise, and accurate information related to systematic reviews. You should focus on understanding the user's query, retrieving relevant data, and presenting it in a way that is easy to comprehend. Your responses should be evidence-based, and you should aim to enhance the user's understanding and support informed decision-making.<<NEW_PROMPT>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/4 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:  25%|██▌       | 1/4 [00:02<00:08,  2.84s/it]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:  50%|█████     | 2/4 [00:04<00:03,  1.99s/it]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:  75%|███████▌  | 3/4 [00:06<00:01,  1.91s/it]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0: 100%|██████████| 4/4 [00:10<00:00,  2.56s/it]\n",
            "Training step 2. Epoch 0: : 2it [00:42, 22.10s/it]INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:TextualGradientDescent prompt for update\n",
            "INFO:textgrad:TextualGradientDescent optimizer response\n",
            "INFO:textgrad:TextualGradientDescent updated text\n",
            "INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:   0%|          | 0/2 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0: 100%|██████████| 2/2 [00:00<00:00, 189.83it/s]\n",
            "INFO:textgrad:LLMCall function forward\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val_performance:  1.0\n",
            "previous_performance:  1.0\n",
            "sys prompt:  <<NEW_PROMPT>>You are an AI assistant specialized in Systematic Reviews. Your primary role is to assist users by providing clear, concise, and accurate information related to systematic reviews. You should focus on understanding the user's query, retrieving relevant data, and presenting it in a way that is easy to comprehend. Your responses should be evidence-based, and you should aim to enhance the user's understanding and support informed decision-making.<<NEW_PROMPT>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/4 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:   0%|          | 0/4 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:   0%|          | 0/4 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0: 100%|██████████| 4/4 [00:00<00:00, 227.95it/s]\n",
            "Training step 3. Epoch 0: : 3it [00:52, 16.33s/it]INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:TextualGradientDescent prompt for update\n",
            "INFO:textgrad:TextualGradientDescent optimizer response\n",
            "INFO:textgrad:TextualGradientDescent updated text\n",
            "INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:   0%|          | 0/2 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0: 100%|██████████| 2/2 [00:00<00:00, 547.81it/s]\n",
            "INFO:textgrad:LLMCall function forward\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val_performance:  1.0\n",
            "previous_performance:  1.0\n",
            "sys prompt:  <<NEW_PROMPT>>You are an AI assistant specialized in Systematic Reviews. Your primary role is to assist users by providing clear, concise, and accurate information related to systematic reviews. You should focus on understanding the user's query, retrieving relevant data, and presenting it in a way that is easy to comprehend. Your responses should be evidence-based, and you should aim to enhance the user's understanding and support informed decision-making.<<NEW_PROMPT>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/4 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:   0%|          | 0/4 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:   0%|          | 0/4 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0: 100%|██████████| 4/4 [00:00<00:00, 410.52it/s]\n",
            "Training step 3. Epoch 0: : 3it [01:03, 21.18s/it]\n",
            "Training step 0. Epoch 1: : 0it [00:00, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:TextualGradientDescent prompt for update\n",
            "INFO:textgrad:TextualGradientDescent optimizer response\n",
            "INFO:textgrad:TextualGradientDescent updated text\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:   0%|          | 0/2 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0: 100%|██████████| 2/2 [00:00<00:00, 212.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val_performance:  1.0\n",
            "previous_performance:  1.0\n",
            "sys prompt:  <<NEW_PROMPT>>You are an AI assistant specialized in Systematic Reviews. Your primary role is to assist users by providing clear, concise, and accurate information related to systematic reviews. You should focus on understanding the user's query, retrieving relevant data, and presenting it in a way that is easy to comprehend. Your responses should be evidence-based, and you should aim to enhance the user's understanding and support informed decision-making.<<NEW_PROMPT>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:   0%|          | 0/4 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:   0%|          | 0/4 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0: 100%|██████████| 4/4 [00:00<00:00, 455.10it/s]\n",
            "Training step 1. Epoch 1: : 1it [00:02,  2.33s/it]INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:TextualGradientDescent prompt for update\n",
            "INFO:textgrad:TextualGradientDescent optimizer response\n",
            "INFO:textgrad:TextualGradientDescent updated text\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:   0%|          | 0/2 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0: 100%|██████████| 2/2 [00:00<00:00, 249.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val_performance:  1.0\n",
            "previous_performance:  1.0\n",
            "sys prompt:  <<NEW_PROMPT>>You are an AI assistant specialized in Systematic Reviews. Your primary role is to assist users by providing clear, concise, and accurate information related to systematic reviews. You should focus on understanding the user's query, retrieving relevant data, and presenting it in a way that is easy to comprehend. Your responses should be evidence-based, and you should aim to enhance the user's understanding and support informed decision-making.<<NEW_PROMPT>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/4 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:   0%|          | 0/4 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:   0%|          | 0/4 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0: 100%|██████████| 4/4 [00:00<00:00, 253.03it/s]\n",
            "Training step 2. Epoch 1: : 2it [00:07,  4.23s/it]INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:TextualGradientDescent prompt for update\n",
            "INFO:textgrad:TextualGradientDescent optimizer response\n",
            "INFO:textgrad:TextualGradientDescent updated text\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:   0%|          | 0/2 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0: 100%|██████████| 2/2 [00:00<00:00, 359.86it/s]\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val_performance:  1.0\n",
            "previous_performance:  1.0\n",
            "sys prompt:  <<NEW_PROMPT>>You are an AI assistant specialized in Systematic Reviews. Your primary role is to assist users by providing clear, concise, and accurate information related to systematic reviews. You should focus on understanding the user's query, retrieving relevant data, and presenting it in a way that is easy to comprehend. Your responses should be evidence-based, and you should aim to enhance the user's understanding and support informed decision-making.<<NEW_PROMPT>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Accuracy: 1.0: 100%|██████████| 4/4 [00:00<00:00, 857.99it/s]\n",
            "Training step 3. Epoch 1: : 3it [00:16,  6.18s/it]INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:TextualGradientDescent prompt for update\n",
            "INFO:textgrad:TextualGradientDescent optimizer response\n",
            "INFO:textgrad:TextualGradientDescent updated text\n",
            "INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:   0%|          | 0/2 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0: 100%|██████████| 2/2 [00:00<00:00, 470.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val_performance:  1.0\n",
            "previous_performance:  1.0\n",
            "sys prompt:  <<NEW_PROMPT>>You are an AI assistant specialized in Systematic Reviews. Your primary role is to assist users by providing clear, concise, and accurate information related to systematic reviews. You should focus on understanding the user's query, retrieving relevant data, and presenting it in a way that is easy to comprehend. Your responses should be evidence-based, and you should aim to enhance the user's understanding and support informed decision-making.<<NEW_PROMPT>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:   0%|          | 0/4 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:   0%|          | 0/4 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0: 100%|██████████| 4/4 [00:00<00:00, 319.70it/s]\n",
            "Training step 3. Epoch 1: : 3it [00:19,  6.50s/it]\n",
            "Training step 0. Epoch 2: : 0it [00:00, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:TextualGradientDescent prompt for update\n",
            "INFO:textgrad:TextualGradientDescent optimizer response\n",
            "INFO:textgrad:TextualGradientDescent updated text\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0: 100%|██████████| 2/2 [00:00<00:00, 177.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val_performance:  1.0\n",
            "previous_performance:  1.0\n",
            "sys prompt:  <<NEW_PROMPT>>You are an AI assistant specialized in Systematic Reviews. Your primary role is to assist users by providing clear, concise, and accurate information related to systematic reviews. You should focus on understanding the user's query, retrieving relevant data, and presenting it in a way that is easy to comprehend. Your responses should be evidence-based, and you should aim to enhance the user's understanding and support informed decision-making.<<NEW_PROMPT>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:textgrad:LLMCall function forward\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:   0%|          | 0/4 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:   0%|          | 0/4 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0: 100%|██████████| 4/4 [00:00<00:00, 388.18it/s]\n",
            "Training step 1. Epoch 2: : 1it [00:03,  3.57s/it]INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:TextualGradientDescent prompt for update\n",
            "INFO:textgrad:TextualGradientDescent optimizer response\n",
            "INFO:textgrad:TextualGradientDescent updated text\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:   0%|          | 0/2 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0: 100%|██████████| 2/2 [00:00<00:00, 309.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val_performance:  1.0\n",
            "previous_performance:  1.0\n",
            "sys prompt:  <<NEW_PROMPT>>You are an AI assistant specialized in Systematic Reviews. Your primary role is to assist users by providing clear, concise, and accurate information related to systematic reviews. You should focus on understanding the user's query, retrieving relevant data, and presenting it in a way that is easy to comprehend. Your responses should be evidence-based, and you should aim to enhance the user's understanding and support informed decision-making.<<NEW_PROMPT>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/4 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:   0%|          | 0/4 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:   0%|          | 0/4 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0: 100%|██████████| 4/4 [00:00<00:00, 145.57it/s]\n",
            "Training step 2. Epoch 2: : 1it [00:03,  3.57s/it]INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:TextualGradientDescent prompt for update\n",
            "INFO:textgrad:TextualGradientDescent optimizer response\n",
            "INFO:textgrad:TextualGradientDescent updated text\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0: 100%|██████████| 2/2 [00:00<00:00, 370.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val_performance:  1.0\n",
            "previous_performance:  1.0\n",
            "sys prompt:  <<NEW_PROMPT>>You are an AI assistant specialized in Systematic Reviews. Your primary role is to assist users by providing clear, concise, and accurate information related to systematic reviews. You should focus on understanding the user's query, retrieving relevant data, and presenting it in a way that is easy to comprehend. Your responses should be evidence-based, and you should aim to enhance the user's understanding and support informed decision-making.<<NEW_PROMPT>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:   0%|          | 0/4 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:   0%|          | 0/4 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0: 100%|██████████| 4/4 [00:00<00:00, 377.21it/s]\n",
            "Training step 3. Epoch 2: : 3it [00:03,  1.03it/s]INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:Idempotent backward\n",
            "INFO:textgrad:TextualGradientDescent prompt for update\n",
            "INFO:textgrad:TextualGradientDescent optimizer response\n",
            "INFO:textgrad:TextualGradientDescent updated text\n",
            "INFO:textgrad:LLMCall function forward\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0: 100%|██████████| 2/2 [00:00<00:00, 318.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val_performance:  1.0\n",
            "previous_performance:  1.0\n",
            "sys prompt:  <<NEW_PROMPT>>You are an AI assistant specialized in Systematic Reviews. Your primary role is to assist users by providing clear, concise, and accurate information related to systematic reviews. You should focus on understanding the user's query, retrieving relevant data, and presenting it in a way that is easy to comprehend. Your responses should be evidence-based, and you should aim to enhance the user's understanding and support informed decision-making.<<NEW_PROMPT>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:textgrad:LLMCall function forward\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0:   0%|          | 0/4 [00:00<?, ?it/s]INFO:textgrad:LLMCall function forward\n",
            "INFO:textgrad:LLMCall function forward\n",
            "Accuracy: 1.0: 100%|██████████| 4/4 [00:00<00:00, 237.24it/s]\n",
            "Training step 3. Epoch 2: : 3it [00:03,  1.25s/it]\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(3):\n",
        "    for steps, (batch_x, batch_y) in enumerate((pbar := tqdm(train_loader, position=0))):\n",
        "        pbar.set_description(f\"Training step {steps}. Epoch {epoch}\")\n",
        "\n",
        "        # zero_grad가 없을 수 있으므로 방어적으로 호출\n",
        "        if hasattr(optimizer, \"zero_grad\"):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        losses = []\n",
        "        for (x, y) in zip(batch_x, batch_y):\n",
        "            x_var = tg.Variable(x, requires_grad=False, role_description=\"query to the language model\")\n",
        "            y_var = tg.Variable(y, requires_grad=False, role_description=\"correct answer for the query\")\n",
        "\n",
        "            pred = model(x_var)  # model은 system_prompt_train을 내부에 참조\n",
        "\n",
        "            try:\n",
        "                loss_var = eval_fn(inputs=dict(prediction=pred, ground_truth_answer=y_var))\n",
        "            except Exception:\n",
        "                loss_var = eval_fn([x_var, y_var, pred])\n",
        "\n",
        "            losses.append(loss_var)\n",
        "\n",
        "        total_loss = tg.sum(losses)\n",
        "        total_loss.backward()\n",
        "\n",
        "        # TextualGradientDescent가 system_prompt_train을 업데이트\n",
        "        optimizer.step()\n",
        "\n",
        "        # 검증 및 되돌리기 로직에 올바른 변수 전달\n",
        "        run_validation_revert(system_prompt_train, results, model, eval_fn, val_set)\n",
        "\n",
        "        # 현재 시스템 프롬프트 문자열 출력\n",
        "        print(\"sys prompt: \", system_prompt_train.get_value())\n",
        "\n",
        "        # 테스트 성능 기록\n",
        "        test_acc = eval_dataset(test_set, eval_fn, model)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "        results[\"prompt\"].append(system_prompt_train.get_value())\n",
        "\n",
        "        if steps == 3:\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dab7a53-e682-478e-9417-15009b495979",
      "metadata": {
        "id": "7dab7a53-e682-478e-9417-15009b495979"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}