{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d7967bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import zipfile\n",
    "# import shutil\n",
    "# import yaml\n",
    "# from pathlib import Path\n",
    "\n",
    "# def extract_and_merge_datasets(zip_path, output_dir=\"merged_dataset\"):\n",
    "\n",
    "#     # Create temporary directory for extraction\n",
    "#     temp_dir = \"temp_extract\"\n",
    "#     os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "#     # Create output directory structure\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     images_dir = os.path.join(output_dir, \"images\")\n",
    "#     labels_dir = os.path.join(output_dir, \"labels\")\n",
    "#     os.makedirs(images_dir, exist_ok=True)\n",
    "#     os.makedirs(labels_dir, exist_ok=True)\n",
    "\n",
    "#     # Extract the zip file\n",
    "#     print(f\"Extracting {zip_path} to {temp_dir}...\")\n",
    "#     with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "#         zip_ref.extractall(temp_dir)\n",
    "\n",
    "#     # Look for data.yaml in the extracted files\n",
    "#     yaml_path = None\n",
    "#     class_names = []\n",
    "#     nc = 0\n",
    "\n",
    "#     for root, dirs, files in os.walk(temp_dir):\n",
    "#         for file in files:\n",
    "#             if file == \"data.yaml\":\n",
    "#                 yaml_path = os.path.join(root, file)\n",
    "#                 break\n",
    "\n",
    "#     # Read class information if data.yaml exists\n",
    "#     if yaml_path:\n",
    "#         with open(yaml_path, 'r') as f:\n",
    "#             yaml_data = yaml.safe_load(f)\n",
    "#             if 'names' in yaml_data:\n",
    "#                 class_names = yaml_data['names']\n",
    "#                 nc = len(class_names)\n",
    "#             elif 'classes' in yaml_data and isinstance(yaml_data['classes'], list):\n",
    "#                 class_names = yaml_data['classes']\n",
    "#                 nc = len(class_names)\n",
    "\n",
    "#     # Find and merge the train, val, test directories\n",
    "#     for split in ['train', 'valid', 'test']:\n",
    "#       split_dir = os.path.join(temp_dir, split)\n",
    "\n",
    "#       if os.path.exists(split_dir):\n",
    "#           print(f\"Processing {split} directory...\")\n",
    "\n",
    "#           # ✅ 들여쓰기 하나만 (if 블록 내부)\n",
    "#           split_images_dir = os.path.join(split_dir, 'images')\n",
    "#           split_labels_dir = os.path.join(split_dir, 'labels')\n",
    "\n",
    "#           if os.path.exists(split_images_dir):\n",
    "#               for img in os.listdir(split_images_dir):\n",
    "#                   src = os.path.join(split_images_dir, img)\n",
    "#                   dst = os.path.join(images_dir, img)\n",
    "#                   os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "#                   shutil.copy2(src, dst)\n",
    "\n",
    "#           if os.path.exists(split_labels_dir):\n",
    "#               for ann in os.listdir(split_labels_dir):\n",
    "#                   src = os.path.join(split_labels_dir, ann)\n",
    "#                   dst = os.path.join(labels_dir, ann)\n",
    "#                   os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "#                   shutil.copy2(src, dst)\n",
    "\n",
    "#     # Create new data.yaml file\n",
    "#     img_count = len(os.listdir(images_dir))\n",
    "#     label_count = len(os.listdir(labels_dir))\n",
    "\n",
    "#     # Determine path to dataset\n",
    "#     dataset_path = os.path.abspath(output_dir)\n",
    "\n",
    "#     # Create new data.yaml\n",
    "#     data_yaml = {\n",
    "#         'path': dataset_path,\n",
    "#         'train': 'images',\n",
    "#         'val': 'images',\n",
    "#         'test': 'images',\n",
    "#         'nc': nc,\n",
    "#         'names': class_names\n",
    "#     }\n",
    "\n",
    "#     yaml_out_path = os.path.join(output_dir, 'data.yaml')\n",
    "#     with open(yaml_out_path, 'w') as f:\n",
    "#         yaml.dump(data_yaml, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "#     # Clean up temporary directory\n",
    "#     print(f\"Cleaning up temporary files...\")\n",
    "#     shutil.rmtree(temp_dir)\n",
    "\n",
    "#     print(f\"Successfully merged datasets into {output_dir}\")\n",
    "#     print(f\"Total images: {img_count}\")\n",
    "#     print(f\"Total labels: {label_count}\")\n",
    "#     print(f\"Created data.yaml with {nc} classes\")\n",
    "#     if nc > 0:\n",
    "#         print(f\"Classes: {', '.join(class_names)}\")\n",
    "#     else:\n",
    "#         print(\"Warning: No class information found in original data.yaml\")\n",
    "# #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db6665f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip_files= ['/content/drive/MyDrive/CV2/Datasets/final-airplane--1.zip']\n",
    "# for zip_path in zip_files:\n",
    "#     output_dir = zip_path.split('.')[0]\n",
    "#     print(f\"Processing {zip_path} -> {output_dir}\")\n",
    "#     print(\"=\"*50)\n",
    "#     extract_and_merge_datasets(zip_path, output_dir)\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef03e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "# import shutil\n",
    "# import yaml\n",
    "# from glob import glob\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# def create_random_splits(data_dir, n_splits=5, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2):\n",
    "#     \"\"\"\n",
    "#     단일 데이터 경로에서 이미지와 라벨 데이터셋을 N개의 랜덤 train/val/test로 분할하고 관련 파일을 생성합니다. #왜냐면 한개로 합쳐서\n",
    "\n",
    "#     Args:\n",
    "#         data_dir (str): 데이터 디렉토리 경로 (images, labels 폴더와 data.yaml 파일을 포함)\n",
    "#         n_splits (int): 생성할 데이터셋 분할 수\n",
    "#         train_ratio (float): 학습 데이터 비율\n",
    "#         val_ratio (float): 검증 데이터 비율\n",
    "#         test_ratio (float): 테스트 데이터 비율\n",
    "#     \"\"\"\n",
    "#     # 비율 합이 1인지 확인\n",
    "#     assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-10, \"비율의 합은 1이어야 합니다\" #assert제대로 안쓰면 error배출 true일때는 넘어감\n",
    "\n",
    "#     # 경로 설정\n",
    "#     images_dir = os.path.join(data_dir, 'images')\n",
    "#     labels_dir = os.path.join(data_dir, 'labels')\n",
    "#     yaml_path = os.path.join(data_dir, 'data.yaml')\n",
    "\n",
    "#     # 디렉토리 존재 확인\n",
    "#     if not os.path.exists(images_dir) or not os.path.exists(labels_dir):\n",
    "#         print(f\"경고: {data_dir}에 images 또는 labels 폴더가 없습니다. 건너뜁니다.\")\n",
    "#         return\n",
    "\n",
    "#     # 이미지 파일 목록 가져오기 (확장자는 필요에 따라 수정)\n",
    "#     image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']\n",
    "#     image_files = []\n",
    "#     for ext in image_extensions:\n",
    "#         image_files.extend(glob(os.path.join(images_dir, ext))) #glob() 함수는 인자로 받은 패턴과 이름이 일치하는 모든 파일과 디렉터리의 리스트를 반환\n",
    "#     #즉 저기에 있는 확장자들은 다 return해준다고 생각하면면됨됨\n",
    "#     # 이미지 파일명만 추출 (확장자 포함)\n",
    "#     image_filenames = [os.path.basename(f) for f in image_files] #자기 위치 폴더면 폴더명명 파일이면 파일명 반환환\n",
    "\n",
    "#     # 해당 이미지 파일에 대응하는 라벨 파일이 있는지 확인\n",
    "#     valid_samples = []\n",
    "#     for img_filename in image_filenames:\n",
    "#         # 이미지 파일명에서 확장자를 제거하고 라벨 파일 확장자 추가\n",
    "#         base_name = os.path.splitext(img_filename)[0]\n",
    "#         label_path = os.path.join(labels_dir, base_name + '.txt')\n",
    "\n",
    "#         # 라벨 파일이 존재하면 유효한 샘플로 간주\n",
    "#         if os.path.exists(label_path):\n",
    "#             valid_samples.append((img_filename, base_name + '.txt')) #일단 라벨을 다 만들었는데 있으면 이게 실행이됨 그래서 이미지랑 라벨이 둘다 있으니까 valid sample에 추가가\n",
    "\n",
    "#     if not valid_samples:\n",
    "#         print(f\"오류: {data_dir}에서 유효한 샘플을 찾을 수 없습니다.\")\n",
    "#         return\n",
    "#     #여기까지 공부부\n",
    "\n",
    "#     # data.yaml 파일 불러오기\n",
    "#     try:\n",
    "#         with open(yaml_path, 'r') as f:\n",
    "#             data_yaml = yaml.safe_load(f)\n",
    "#             nc = data_yaml.get('nc', 0)\n",
    "#             names = data_yaml.get('names', [])\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"경고: {yaml_path} 파일을 찾을 수 없습니다. nc와 names는 기본값으로 설정됩니다.\")\n",
    "#         nc = 0\n",
    "#         names = []\n",
    "\n",
    "#     print(f\"{data_dir}: {len(valid_samples)}개의 유효한 샘플을 찾았습니다.\")\n",
    "\n",
    "#     # N개의 랜덤 분할 생성\n",
    "#     for iter_idx in range(1, n_splits + 1):\n",
    "#         # 데이터 세트 분할\n",
    "#         random.shuffle(valid_samples) #리스트에 있는 random sample을 알아서 나눔\n",
    "\n",
    "\n",
    "#         # 먼저 train과 temp(val+test) 분할\n",
    "#         train_samples, temp_samples = train_test_split(\n",
    "#             valid_samples,\n",
    "#             train_size=train_ratio,\n",
    "#             test_size=val_ratio + test_ratio,\n",
    "#             random_state=42 + iter_idx  # 각 반복마다 다른 시드 사용\n",
    "#         )\n",
    "\n",
    "#         # temp를 val과 test로 분할\n",
    "#         val_ratio_adjusted = val_ratio / (val_ratio + test_ratio)\n",
    "#         val_samples, test_samples = train_test_split(\n",
    "#             temp_samples,\n",
    "#             train_size=val_ratio_adjusted,\n",
    "#             test_size=1 - val_ratio_adjusted,\n",
    "#             random_state=42 + iter_idx\n",
    "#         )\n",
    "\n",
    "#         # 텍스트 파일 생성\n",
    "#         train_txt_path = os.path.join(data_dir, f'train_iter_{iter_idx:02d}.txt')\n",
    "#         val_txt_path = os.path.join(data_dir, f'val_iter_{iter_idx:02d}.txt')\n",
    "#         test_txt_path = os.path.join(data_dir, f'test_iter_{iter_idx:02d}.txt')\n",
    "\n",
    "#         # 학습 데이터 파일 쓰기\n",
    "#         with open(train_txt_path, 'w') as f:\n",
    "#             for img_file, _ in train_samples:\n",
    "#                 img_path = os.path.join('images', img_file)  # 상대 경로 사용\n",
    "#                 f.write(f\"{img_path}\\n\")\n",
    "\n",
    "#         # 검증 데이터 파일 쓰기\n",
    "#         with open(val_txt_path, 'w') as f:\n",
    "#             for img_file, _ in val_samples:\n",
    "#                 img_path = os.path.join('images', img_file)  # 상대 경로 사용\n",
    "#                 f.write(f\"{img_path}\\n\")\n",
    "\n",
    "#         # 테스트 데이터 파일 쓰기\n",
    "#         with open(test_txt_path, 'w') as f:\n",
    "#             for img_file, _ in test_samples:\n",
    "#                 img_path = os.path.join('images', img_file)  # 상대 경로 사용\n",
    "#                 f.write(f\"{img_path}\\n\")\n",
    "\n",
    "#         # YAML 파일 생성\n",
    "#         output_yaml_path = os.path.join(data_dir, f'data_iter_{iter_idx:02d}.yaml')\n",
    "#         yaml_content = {\n",
    "#             'train': f'train_iter_{iter_idx:02d}.txt',  # 상대 경로 사용\n",
    "#             'val': f'val_iter_{iter_idx:02d}.txt',      # 상대 경로 사용\n",
    "#             'test': f'test_iter_{iter_idx:02d}.txt',    # 상대 경로 사용\n",
    "#             'nc': nc,\n",
    "#             'names': names\n",
    "#         }\n",
    "\n",
    "#         with open(output_yaml_path, 'w') as f:\n",
    "#             yaml.dump(yaml_content, f, default_flow_style=False)\n",
    "\n",
    "#         print(f\"{data_dir} - Iteration {iter_idx} 완료: {len(train_samples)} 학습, {len(val_samples)} 검증, {len(test_samples)} 테스트\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d34ca3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dirs = ['/content/drive/MyDrive/CV2/Datasets/final-airplane--1']\n",
    "# n_splits = 10\n",
    "# train_ratio = 0.6\n",
    "# val_ratio = 0.2\n",
    "# test_ratio = 0.2\n",
    "\n",
    "# for data_dir in data_dirs:\n",
    "#         print(f\"\\n처리 중: {data_dir}\")\n",
    "#         create_random_splits(data_dir, n_splits, train_ratio, val_ratio, test_ratio) #여기까지 data전처지지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2bb8ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import albumentations as A\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # 설정\n",
    "# txt_base_dir = '/content/drive/MyDrive/01/Datasets/final-airplane--1' #로컬에 맞춰서 개선\n",
    "# input_img_dir = os.path.join(txt_base_dir, 'images')\n",
    "# input_lbl_dir = os.path.join(txt_base_dir, 'labels')\n",
    "# output_img_dir = input_img_dir\n",
    "# output_lbl_dir = input_lbl_dir\n",
    "\n",
    "# # 증강 정의\n",
    "# transform = A.Compose([\n",
    "#     A.RandomSizedCrop(min_max_height=(300, 500), size=(640, 640), p=0.3),\n",
    "#     A.Rotate(limit=30, p=0.4),\n",
    "#     A.HorizontalFlip(p=0.5),\n",
    "#     A.HueSaturationValue(p=0.4),\n",
    "#     A.RGBShift(p=0.3),\n",
    "#     A.RandomBrightnessContrast(brightness_limit=(-0.4, 0.1), contrast_limit=0.3, p=0.4),\n",
    "#     A.Resize(height=640, width=640, p=1.0),\n",
    "#     A.GaussianBlur(p=0.1)\n",
    "# ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels'], min_visibility=0.3))\n",
    "\n",
    "\n",
    "# # train_iter_01 ~ train_iter_10\n",
    "# for i in range(1, 11):\n",
    "#     iter_id = f\"{i:02d}\"\n",
    "#     txt_name = f\"train_iter_{iter_id}.txt\"\n",
    "#     txt_path = os.path.join(txt_base_dir, txt_name)\n",
    "\n",
    "#     if not os.path.exists(txt_path):\n",
    "#         print(f\"❌ {txt_name} 없음 - 건너뜀\")\n",
    "#         continue\n",
    "\n",
    "#     with open(txt_path, 'r') as f:\n",
    "#         image_paths = [line.strip() for line in f.readlines()]\n",
    "\n",
    "#     print(f\"🔄 {txt_name} 처리 중 ({len(image_paths)}장)\")\n",
    "\n",
    "#     new_lines = []\n",
    "\n",
    "#     for img_path in tqdm(image_paths):\n",
    "#         img_name = os.path.basename(img_path)\n",
    "#         img_stem = os.path.splitext(img_name)[0]\n",
    "#         input_img_path = os.path.join(input_img_dir, img_name)\n",
    "#         input_lbl_path = os.path.join(input_lbl_dir, f\"{img_stem}.txt\")\n",
    "\n",
    "#         image = cv2.imread(input_img_path)\n",
    "#         if image is None:\n",
    "#             print(f\"❌ 이미지 로드 실패: {input_img_path}\")\n",
    "#             continue\n",
    "\n",
    "#         # 라벨 로드\n",
    "#         bboxes = []\n",
    "#         class_labels = []\n",
    "#         if os.path.exists(input_lbl_path):\n",
    "#             with open(input_lbl_path, 'r') as f:\n",
    "#                 for line in f:\n",
    "#                     parts = line.strip().split()\n",
    "#                     if len(parts) == 5:\n",
    "#                         cls, x, y, w, h = parts\n",
    "#                         bboxes.append([float(x), float(y), float(w), float(h)])\n",
    "#                         class_labels.append(int(cls))\n",
    "\n",
    "#         try:\n",
    "#             transformed = transform(image=image, bboxes=bboxes, class_labels=class_labels)\n",
    "#         except Exception as e:\n",
    "#             print(f\"⚠️ bbox 오류로 해당 이미지 건너뜀: {img_path}\")\n",
    "#             continue\n",
    "\n",
    "#         aug_image = transformed['image']\n",
    "#         aug_bboxes = transformed['bboxes']\n",
    "#         aug_labels = transformed['class_labels']\n",
    "\n",
    "#         if not aug_bboxes:\n",
    "#             continue\n",
    "\n",
    "#         # 저장 파일명\n",
    "#         aug_img_name = f\"aug_{img_name}\"\n",
    "#         aug_lbl_name = f\"aug_{img_stem}.txt\"\n",
    "#         save_img_path = os.path.join(output_img_dir, aug_img_name)\n",
    "#         save_lbl_path = os.path.join(output_lbl_dir, aug_lbl_name)\n",
    "\n",
    "#         # 저장\n",
    "#         cv2.imwrite(save_img_path, aug_image)\n",
    "#         with open(save_lbl_path, 'w') as f:\n",
    "#             for bbox, cls in zip(aug_bboxes, aug_labels):\n",
    "#                 f.write(f\"{cls} {' '.join(f'{x:.6f}' for x in bbox)}\\n\")\n",
    "\n",
    "#         # txt에 기록할 새로운 이미지 경로\n",
    "#         new_lines.append(save_img_path)\n",
    "\n",
    "#     # 기존 train_iter_xx.txt에 덧붙이기\n",
    "#     with open(txt_path, 'a') as f:\n",
    "#         for new_line in new_lines:\n",
    "#             f.write(f\"{new_line}\\n\")\n",
    "\n",
    "#     print(f\"✅ {txt_name} 에 증강 이미지 {len(new_lines)}장 추가 완료\")\n",
    "\n",
    "# print(\"🎉 전체 train_iter_xx 파일에 증강 이미지 경로 추가 완료\")\n",
    "#이건 전체과정 한번에 다한거임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a22a04-1665-4ea8-b1a4-e0172ce1a30a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CBAM registered: True\n",
      "New https://pypi.org/project/ultralytics/8.3.114 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.96  Python-3.10.16 torch-2.6.0+cpu CPU (13th Gen Intel Core(TM) i5-1335U)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8_1n.yaml, data=Datasets/Crown Detection/data_iter_01.yaml, epochs=1, time=None, patience=20, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=tmp, name=250423_112117_yolov8n_Crown Detection_Iter_1, exist_ok=True, pretrained=False, optimizer=AdamW, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=False, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.001, lrf=0.01, momentum=0.0001, weight_decay=0.0001, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=tmp\\250423_112117_yolov8n_Crown Detection_Iter_1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       696  ultralytics.nn.modules.conv.Conv             [3, 24, 3, 2]                 \n",
      "  1                  -1  1      8720  ultralytics.nn.modules.conv.Conv             [24, 40, 3, 2]                \n",
      "  2                  -1  1       258  models.common.CBAM                           [40]                          \n",
      "  3                  -1  1     11440  ultralytics.nn.modules.block.C2f             [40, 40, 1, True]             \n",
      "  4                  -1  1     28960  ultralytics.nn.modules.conv.Conv             [40, 80, 3, 2]                \n",
      "  5                  -1  1       898  models.common.CBAM                           [80]                          \n",
      "  6                  -1  2     77440  ultralytics.nn.modules.block.C2f             [80, 80, 2, True]             \n",
      "  7                  -1  1    109744  ultralytics.nn.modules.conv.Conv             [80, 152, 3, 2]               \n",
      "  8                  -1  2    278464  ultralytics.nn.modules.block.C2f             [152, 152, 2, True]           \n",
      "  9                  -1  1    405520  ultralytics.nn.modules.conv.Conv             [152, 296, 3, 2]              \n",
      " 10                  -1  1     10754  models.common.CBAM                           [296]                         \n",
      " 11                  -1  1    615088  ultralytics.nn.modules.block.C2f             [296, 296, 1, True]           \n",
      " 12                  -1  1    219928  ultralytics.nn.modules.block.SPPF            [296, 296, 5]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    207632  ultralytics.nn.modules.block.C2f             [448, 152, 1]                 \n",
      " 16                  -1  1      2834  models.common.CBAM                           [152]                         \n",
      " 17                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 18             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     57440  ultralytics.nn.modules.block.C2f             [232, 80, 1]                  \n",
      " 20                  -1  1       898  models.common.CBAM                           [80]                          \n",
      " 21                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 22             [-1, 3]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 23                  -1  1     14640  ultralytics.nn.modules.block.C2f             [120, 40, 1]                  \n",
      " 24                  -1  1     14480  ultralytics.nn.modules.conv.Conv             [40, 40, 3, 2]                \n",
      " 25            [-1, 20]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 26                  -1  1     48480  ultralytics.nn.modules.block.C2f             [120, 80, 1]                  \n",
      " 27                  -1  1     57760  ultralytics.nn.modules.conv.Conv             [80, 80, 3, 2]                \n",
      " 28            [-1, 16]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 29                  -1  1    174800  ultralytics.nn.modules.block.C2f             [232, 152, 1]                 \n",
      " 30                  -1  1    208240  ultralytics.nn.modules.conv.Conv             [152, 152, 3, 2]              \n",
      " 31            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 32                  -1  1    660080  ultralytics.nn.modules.block.C2f             [448, 296, 1]                 \n",
      " 33    [23, 26, 29, 32]  1    755188  ultralytics.nn.modules.head.Detect           [1, [40, 80, 152, 296]]       \n",
      "YOLOv8_1n summary: 195 layers, 3,970,382 parameters, 3,970,366 gradients, 16.7 GFLOPs\n",
      "\n",
      "Freezing layer 'model.33.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning Datasets\\Crown Detection\\labels... 348 images, 68 backgrounds, 0 corrupt: 100%|██████████| 348/348 [00:00<00:00, 1844.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: Datasets\\Crown Detection\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning Datasets\\Crown Detection\\labels... 116 images, 27 backgrounds, 0 corrupt: 100%|██████████| 116/116 [00:00<00:00, 1738.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: Datasets\\Crown Detection\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to tmp\\250423_112117_yolov8n_Crown Detection_Iter_1\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.0001) with parameter groups 70 weight(decay=0.0), 94 weight(decay=0.0001), 78 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mtmp\\250423_112117_yolov8n_Crown Detection_Iter_1\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1         0G      3.098      3.436      3.486         92        640: 100%|██████████| 22/22 [03:19<00:00,  9.08s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:28<00:00,  7.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        116        783    0.00196     0.0549    0.00608    0.00385\n",
      "\n",
      "1 epochs completed in 0.064 hours.\n",
      "Optimizer stripped from tmp\\250423_112117_yolov8n_Crown Detection_Iter_1\\weights\\last.pt, 8.4MB\n",
      "Optimizer stripped from tmp\\250423_112117_yolov8n_Crown Detection_Iter_1\\weights\\best.pt, 8.4MB\n",
      "\n",
      "Validating tmp\\250423_112117_yolov8n_Crown Detection_Iter_1\\weights\\best.pt...\n",
      "Ultralytics 8.3.96  Python-3.10.16 torch-2.6.0+cpu CPU (13th Gen Intel Core(TM) i5-1335U)\n",
      "YOLOv8_1n summary (fused): 125 layers, 3,963,906 parameters, 0 gradients, 16.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:20<00:00,  5.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        116        783    0.00192     0.0524    0.00207    0.00105\n",
      "Speed: 2.7ms preprocess, 168.2ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mtmp\\250423_112117_yolov8n_Crown Detection_Iter_1\u001b[0m\n",
      "Transferred 452/452 items from pretrained weights\n",
      "결과가 250423_112117_submission_1_20214173_Iter_1_detection_results.json에 저장되었습니다.\n",
      "New https://pypi.org/project/ultralytics/8.3.114 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.96  Python-3.10.16 torch-2.6.0+cpu CPU (13th Gen Intel Core(TM) i5-1335U)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov11n.yaml, data=Datasets/Crown Detection/data_iter_01.yaml, epochs=1, time=None, patience=20, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=tmp, name=250423_112553_yolov11n_Crown Detection_Iter_1, exist_ok=True, pretrained=False, optimizer=AdamW, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=False, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.001, lrf=0.01, momentum=0.0001, weight_decay=0.0001, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=tmp\\250423_112553_yolov11n_Crown Detection_Iter_1\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLOv11n summary: 181 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning Datasets\\Crown Detection\\labels... 348 images, 68 backgrounds, 0 corrupt: 100%|██████████| 348/348 [00:00<00:00, 1982.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: Datasets\\Crown Detection\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning Datasets\\Crown Detection\\labels... 116 images, 27 backgrounds, 0 corrupt: 100%|██████████| 116/116 [00:00<00:00, 1747.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: Datasets\\Crown Detection\\labels.cache\n",
      "Plotting labels to tmp\\250423_112553_yolov11n_Crown Detection_Iter_1\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.0001) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0001), 87 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mtmp\\250423_112553_yolov11n_Crown Detection_Iter_1\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1         0G      3.594      3.104      3.436         92        640: 100%|██████████| 22/22 [01:39<00:00,  4.54s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:10<00:00,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        116        783    0.00243     0.0677    0.00198   0.000807\n",
      "\n",
      "1 epochs completed in 0.031 hours.\n",
      "Optimizer stripped from tmp\\250423_112553_yolov11n_Crown Detection_Iter_1\\weights\\last.pt, 5.5MB\n",
      "Optimizer stripped from tmp\\250423_112553_yolov11n_Crown Detection_Iter_1\\weights\\best.pt, 5.5MB\n",
      "\n",
      "Validating tmp\\250423_112553_yolov11n_Crown Detection_Iter_1\\weights\\best.pt...\n",
      "Ultralytics 8.3.96  Python-3.10.16 torch-2.6.0+cpu CPU (13th Gen Intel Core(TM) i5-1335U)\n",
      "YOLOv11n summary (fused): 100 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:08<00:00,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        116        783    0.00248      0.069    0.00339    0.00149\n",
      "Speed: 0.7ms preprocess, 67.6ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1mtmp\\250423_112553_yolov11n_Crown Detection_Iter_1\u001b[0m\n",
      "결과가 250423_112553_submission_2_20214173_Iter_1_detection_results.json에 저장되었습니다.\n",
      "New https://pypi.org/project/ultralytics/8.3.114 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.96  Python-3.10.16 torch-2.6.0+cpu CPU (13th Gen Intel Core(TM) i5-1335U)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov12n.yaml, data=Datasets/Crown Detection/data_iter_01.yaml, epochs=1, time=None, patience=20, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=tmp, name=250423_112810_yolov12n_Crown Detection_Iter_1, exist_ok=True, pretrained=False, optimizer=AdamW, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=False, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.001, lrf=0.01, momentum=0.0001, weight_decay=0.0001, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=tmp\\250423_112810_yolov12n_Crown Detection_Iter_1\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  2    180864  ultralytics.nn.modules.block.A2C2f           [128, 128, 2, True, 4]        \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  2    689408  ultralytics.nn.modules.block.A2C2f           [256, 256, 2, True, 1]        \n",
      "  9                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 10             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 11                  -1  1     86912  ultralytics.nn.modules.block.A2C2f           [384, 128, 1, False, -1]      \n",
      " 12                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 13             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 14                  -1  1     24000  ultralytics.nn.modules.block.A2C2f           [256, 64, 1, False, -1]       \n",
      " 15                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 16            [-1, 11]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  1     74624  ultralytics.nn.modules.block.A2C2f           [192, 128, 1, False, -1]      \n",
      " 18                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 19             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 21        [14, 17, 20]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLOv12n summary: 272 layers, 2,568,243 parameters, 2,568,227 gradients, 6.5 GFLOPs\n",
      "\n",
      "Freezing layer 'model.21.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning Datasets\\Crown Detection\\labels... 348 images, 68 backgrounds, 0 corrupt: 100%|██████████| 348/348 [00:00<00:00, 2063.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: Datasets\\Crown Detection\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning Datasets\\Crown Detection\\labels... 116 images, 27 backgrounds, 0 corrupt: 100%|██████████| 116/116 [00:00<00:00, 1886.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: Datasets\\Crown Detection\\labels.cache\n",
      "Plotting labels to tmp\\250423_112810_yolov12n_Crown Detection_Iter_1\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.0001) with parameter groups 113 weight(decay=0.0), 120 weight(decay=0.0001), 119 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mtmp\\250423_112810_yolov12n_Crown Detection_Iter_1\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1         0G      3.641      3.347      3.511         92        640: 100%|██████████| 22/22 [01:50<00:00,  5.03s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:20<00:00,  5.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        116        783    0.00391      0.174     0.0034    0.00122\n",
      "\n",
      "1 epochs completed in 0.037 hours.\n",
      "Optimizer stripped from tmp\\250423_112810_yolov12n_Crown Detection_Iter_1\\weights\\last.pt, 5.5MB\n",
      "Optimizer stripped from tmp\\250423_112810_yolov12n_Crown Detection_Iter_1\\weights\\best.pt, 5.5MB\n",
      "\n",
      "Validating tmp\\250423_112810_yolov12n_Crown Detection_Iter_1\\weights\\best.pt...\n",
      "Ultralytics 8.3.96  Python-3.10.16 torch-2.6.0+cpu CPU (13th Gen Intel Core(TM) i5-1335U)\n",
      "YOLOv12n summary (fused): 159 layers, 2,556,923 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:17<00:00,  4.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        116        783    0.00405       0.18    0.00443    0.00153\n",
      "Speed: 0.8ms preprocess, 76.5ms inference, 0.0ms loss, 67.2ms postprocess per image\n",
      "Results saved to \u001b[1mtmp\\250423_112810_yolov12n_Crown Detection_Iter_1\u001b[0m\n",
      "결과가 250423_112810_submission_3_20214173_Iter_1_detection_results.json에 저장되었습니다.\n",
      "✅ CBAM registered: True\n",
      "New https://pypi.org/project/ultralytics/8.3.114 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.96  Python-3.10.16 torch-2.6.0+cpu CPU (13th Gen Intel Core(TM) i5-1335U)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8_1n.yaml, data=Datasets/Crown Detection/data_iter_02.yaml, epochs=1, time=None, patience=20, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=tmp, name=250423_113057_yolov8n_Crown Detection_Iter_2, exist_ok=True, pretrained=False, optimizer=AdamW, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=False, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.001, lrf=0.01, momentum=0.0001, weight_decay=0.0001, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=tmp\\250423_113057_yolov8n_Crown Detection_Iter_2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       696  ultralytics.nn.modules.conv.Conv             [3, 24, 3, 2]                 \n",
      "  1                  -1  1      8720  ultralytics.nn.modules.conv.Conv             [24, 40, 3, 2]                \n",
      "  2                  -1  1       258  models.common.CBAM                           [40]                          \n",
      "  3                  -1  1     11440  ultralytics.nn.modules.block.C2f             [40, 40, 1, True]             \n",
      "  4                  -1  1     28960  ultralytics.nn.modules.conv.Conv             [40, 80, 3, 2]                \n",
      "  5                  -1  1       898  models.common.CBAM                           [80]                          \n",
      "  6                  -1  2     77440  ultralytics.nn.modules.block.C2f             [80, 80, 2, True]             \n",
      "  7                  -1  1    109744  ultralytics.nn.modules.conv.Conv             [80, 152, 3, 2]               \n",
      "  8                  -1  2    278464  ultralytics.nn.modules.block.C2f             [152, 152, 2, True]           \n",
      "  9                  -1  1    405520  ultralytics.nn.modules.conv.Conv             [152, 296, 3, 2]              \n",
      " 10                  -1  1     10754  models.common.CBAM                           [296]                         \n",
      " 11                  -1  1    615088  ultralytics.nn.modules.block.C2f             [296, 296, 1, True]           \n",
      " 12                  -1  1    219928  ultralytics.nn.modules.block.SPPF            [296, 296, 5]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    207632  ultralytics.nn.modules.block.C2f             [448, 152, 1]                 \n",
      " 16                  -1  1      2834  models.common.CBAM                           [152]                         \n",
      " 17                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 18             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     57440  ultralytics.nn.modules.block.C2f             [232, 80, 1]                  \n",
      " 20                  -1  1       898  models.common.CBAM                           [80]                          \n",
      " 21                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 22             [-1, 3]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 23                  -1  1     14640  ultralytics.nn.modules.block.C2f             [120, 40, 1]                  \n",
      " 24                  -1  1     14480  ultralytics.nn.modules.conv.Conv             [40, 40, 3, 2]                \n",
      " 25            [-1, 20]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 26                  -1  1     48480  ultralytics.nn.modules.block.C2f             [120, 80, 1]                  \n",
      " 27                  -1  1     57760  ultralytics.nn.modules.conv.Conv             [80, 80, 3, 2]                \n",
      " 28            [-1, 16]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 29                  -1  1    174800  ultralytics.nn.modules.block.C2f             [232, 152, 1]                 \n",
      " 30                  -1  1    208240  ultralytics.nn.modules.conv.Conv             [152, 152, 3, 2]              \n",
      " 31            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 32                  -1  1    660080  ultralytics.nn.modules.block.C2f             [448, 296, 1]                 \n",
      " 33    [23, 26, 29, 32]  1    755188  ultralytics.nn.modules.head.Detect           [1, [40, 80, 152, 296]]       \n",
      "YOLOv8_1n summary: 195 layers, 3,970,382 parameters, 3,970,366 gradients, 16.7 GFLOPs\n",
      "\n",
      "Freezing layer 'model.33.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning Datasets\\Crown Detection\\labels... 348 images, 69 backgrounds, 0 corrupt: 100%|██████████| 348/348 [00:00<00:00, 1920.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: Datasets\\Crown Detection\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning Datasets\\Crown Detection\\labels... 116 images, 24 backgrounds, 0 corrupt: 100%|██████████| 116/116 [00:00<00:00, 2367.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: Datasets\\Crown Detection\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to tmp\\250423_113057_yolov8n_Crown Detection_Iter_2\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.0001) with parameter groups 70 weight(decay=0.0), 94 weight(decay=0.0001), 78 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mtmp\\250423_113057_yolov8n_Crown Detection_Iter_2\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1         0G      3.077      3.426      3.492        133        640: 100%|██████████| 22/22 [03:38<00:00,  9.92s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:49<00:00, 12.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        116        730    0.00212      0.074    0.00136   0.000559\n",
      "\n",
      "1 epochs completed in 0.075 hours.\n",
      "Optimizer stripped from tmp\\250423_113057_yolov8n_Crown Detection_Iter_2\\weights\\last.pt, 8.4MB\n",
      "Optimizer stripped from tmp\\250423_113057_yolov8n_Crown Detection_Iter_2\\weights\\best.pt, 8.4MB\n",
      "\n",
      "Validating tmp\\250423_113057_yolov8n_Crown Detection_Iter_2\\weights\\best.pt...\n",
      "Ultralytics 8.3.96  Python-3.10.16 torch-2.6.0+cpu CPU (13th Gen Intel Core(TM) i5-1335U)\n",
      "YOLOv8_1n summary (fused): 125 layers, 3,963,906 parameters, 0 gradients, 16.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:36<00:00,  9.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        116        730    0.00212      0.074    0.00201   0.000681\n",
      "Speed: 4.5ms preprocess, 287.0ms inference, 0.0ms loss, 4.7ms postprocess per image\n",
      "Results saved to \u001b[1mtmp\\250423_113057_yolov8n_Crown Detection_Iter_2\u001b[0m\n",
      "Transferred 452/452 items from pretrained weights\n",
      "결과가 250423_113057_submission_1_20214173_Iter_2_detection_results.json에 저장되었습니다.\n",
      "New https://pypi.org/project/ultralytics/8.3.114 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.96  Python-3.10.16 torch-2.6.0+cpu CPU (13th Gen Intel Core(TM) i5-1335U)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov11n.yaml, data=Datasets/Crown Detection/data_iter_02.yaml, epochs=1, time=None, patience=20, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=tmp, name=250423_113643_yolov11n_Crown Detection_Iter_2, exist_ok=True, pretrained=False, optimizer=AdamW, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=False, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.001, lrf=0.01, momentum=0.0001, weight_decay=0.0001, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=tmp\\250423_113643_yolov11n_Crown Detection_Iter_2\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLOv11n summary: 181 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning Datasets\\Crown Detection\\labels... 348 images, 69 backgrounds, 0 corrupt: 100%|██████████| 348/348 [00:00<00:00, 2171.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: Datasets\\Crown Detection\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning Datasets\\Crown Detection\\labels... 116 images, 24 backgrounds, 0 corrupt: 100%|██████████| 116/116 [00:00<00:00, 2150.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: Datasets\\Crown Detection\\labels.cache\n",
      "Plotting labels to tmp\\250423_113643_yolov11n_Crown Detection_Iter_2\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.0001) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0001), 87 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mtmp\\250423_113643_yolov11n_Crown Detection_Iter_2\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1         0G      3.626       3.23      3.562        122        640:  77%|███████▋  | 17/22 [01:52<00:24,  4.95s/it]"
     ]
    }
   ],
   "source": [
    "from competition_utils import *\n",
    "# submission_1_20214173.py 상단에 추가!\n",
    "from models.common import CBAM  # common.py에 정의한 CBAM 경로\n",
    "\n",
    "# 등록: YAML에서 'CBAM'이라는 이름으로 쓸 수 있도록 연결\n",
    "import ultralytics.nn.modules as modules\n",
    "modules.CBAM = CBAM\n",
    "\n",
    "# 제출 함수를 리스트로 정의 (submission_N_학번)\n",
    "# 깃허브에는 최대 3개 업로드 데모시 하이퍼파라미터 튜닝 후 1개만 제출\n",
    "submission_functions = ['submission_1_20214173', 'submission_2_20214173', 'submission_3_20214173']  \n",
    "for submission_function in submission_functions:\n",
    "    exec(f\"from {submission_function} import {submission_function}\")\n",
    "\n",
    "# 분석 방향에 따라 iteration 수 수정\n",
    "# 평가시에는 데모와 다른 스플릿 시드를 만들어 [1, 10]로 진행 예정\n",
    "iterations = [1, 10] \n",
    "\n",
    "# 'Crown Detection'는 예제 데이터셋, 데모 및 평가시에는 'CV_Competition'\n",
    "Dataset_Name = 'Crown Detection' \n",
    "\n",
    "# 결과를 모아 하나의 csv 파일에 저장\n",
    "results_df = pd.DataFrame(columns=[\n",
    "    'Experiment Time', 'Iteration', 'Submission Function', \n",
    "    'IoU', 'Dice', 'Precision', 'Recall', 'Output Json Path',\n",
    "])\n",
    "csv_filename = f\"Evaluation_Results_{datetime.now().strftime('%y%m%d_%H%M%S')}.csv\"\n",
    "\n",
    "# 서로 다른 iteration, submission function으로 실험 진행\n",
    "for iteration in range(iterations[0], iterations[1] + 1):\n",
    "    yaml_path = f'Datasets/{Dataset_Name}/data_iter_{iteration:02d}.yaml'\n",
    "    for submission_function in submission_functions:\n",
    "        ex_time = datetime.now().strftime('%y%m%d_%H%M%S')\n",
    "        output_json_path = f\"{ex_time}_{submission_function}_Iter_{iteration}_detection_results.json\"\n",
    "        globals()[submission_function](yaml_path, output_json_path)\n",
    "        labels_dir = f\"Datasets/{Dataset_Name}/labels\"  \n",
    "        vis_output_dir = f\"{ex_time}_visualization_results\"  \n",
    "        image_level_result_path =f\"{ex_time}_{submission_function}_Iter_{iteration}_image_level_results.json\"\n",
    "        stats = eval_and_vis(yaml_path, output_json_path, labels_dir, image_level_result_path, vis_output_dir, vis=False) # 분석 방향에 따라 vis=True 설정\n",
    "        new_row = {\n",
    "            'Experiment Time': ex_time,\n",
    "            'Iteration': iteration,\n",
    "            'Submission Function': submission_function,\n",
    "            'IoU': stats['IoU']['avg'],\n",
    "            'Dice': stats['Dice']['avg'],\n",
    "            'Precision': stats['Precision']['avg'],\n",
    "            'Recall': stats['Recall']['avg'],\n",
    "            'Output Json Pa th': output_json_path,\n",
    "        }\n",
    "        results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        results_df.to_csv(csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42711cc-d6dc-49db-b7cd-d9ebc4a9f596",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Evaluation_Results_250423_104555.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m decimal_places \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m     14\u001b[0m transpose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[43mmake_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_measures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreference_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_fmt_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignificance_levels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecimal_places\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\2023user\\Downloads\\CV_midterm_competition_code_v2\\competition_utils.py:438\u001b[0m, in \u001b[0;36mmake_tables\u001b[1;34m(input_csv_path, keep_columns, keep_measures, reduction, row, column, null_column, custom_fmt_template, significance_levels, decimal_places, transpose)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmake_tables\u001b[39m(input_csv_path, keep_columns, keep_measures, reduction, row, column, null_column, custom_fmt_template,\n\u001b[0;32m    435\u001b[0m                 significance_levels,decimal_places, transpose\n\u001b[0;32m    436\u001b[0m                ):\n\u001b[1;32m--> 438\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mreshape_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_csv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_measures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMeasure Type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m     rename_row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(df[row]\u001b[38;5;241m.\u001b[39munique(), df[row]\u001b[38;5;241m.\u001b[39munique()))\n\u001b[0;32m    440\u001b[0m     rename_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(df[column]\u001b[38;5;241m.\u001b[39munique(), df[column]\u001b[38;5;241m.\u001b[39munique()))\n",
      "File \u001b[1;32mc:\\Users\\2023user\\Downloads\\CV_midterm_competition_code_v2\\competition_utils.py:258\u001b[0m, in \u001b[0;36mreshape_df\u001b[1;34m(input_csv_path, keep_columns, column_groups, groups_name, value_name)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreshape_df\u001b[39m(input_csv_path, keep_columns, column_groups, groups_name, value_name):\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;66;03m# Load CSV file\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_csv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcp949\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Drop rows with missing values in specified columns\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39mkeep_columns \u001b[38;5;241m+\u001b[39m column_groups)\n",
      "File \u001b[1;32mc:\\Users\\2023user\\anaconda3\\envs\\ComputerVision\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\2023user\\anaconda3\\envs\\ComputerVision\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\2023user\\anaconda3\\envs\\ComputerVision\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\2023user\\anaconda3\\envs\\ComputerVision\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\2023user\\anaconda3\\envs\\ComputerVision\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Evaluation_Results_250423_104555.csv'"
     ]
    }
   ],
   "source": [
    "# 분석 방향에 따라 전체 결과를 모아 평균-표준편차-통계테스트 결과 저장 \n",
    "\n",
    "keep_columns = ['Iteration','Submission Function']\n",
    "keep_measures = ['IoU','Dice','Precision','Recall']  \n",
    "\n",
    "reduction = 'Iteration'\n",
    "row = 'Measure Type'\n",
    "column = 'Submission Function'\n",
    "reference_column = 'submission_1_20214173'  # 통계 테스트 기준이 되는 submission function\n",
    "\n",
    "custom_fmt_template = '{mean_fmt} ± {std_fmt} {significance}'\n",
    "significance_levels = [0.1, 0.05, 0.01]\n",
    "decimal_places = 3\n",
    "transpose = True\n",
    "make_tables(csv_filename, keep_columns, keep_measures, reduction, row, column,  \n",
    "            reference_column, custom_fmt_template, significance_levels, decimal_places, transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046f0a89-b0b6-4406-9500-24156dabcfc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ComputerVision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
